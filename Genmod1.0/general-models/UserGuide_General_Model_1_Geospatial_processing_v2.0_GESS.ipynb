{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create MODFLOW-grid-based tiff files and model_grid.csv file from GIS data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project specific variables are imported in the model_spec.py and gen_mod_dict.py files that must be included in the notebook directory. The first first includes pathnames to data sources that will be different for each user. The second file includes a dictionary of model-specific information such as cell size, default hydraulic parameter values, and scenario defintion (e.g. include bedrock, number of layers, etc.). There are examples in the repository. Run the following cells up to the \"Run to here\" cell to get a pull-down menu of models in the model_dict. Then, without re-running that cell, run all the remaining cells. Re-running the following cell would re-set the model to the first one in the list, which you probably don't want. If you use the notebook option to run all cells below, it runs the cell you're in, so if you use that option, move to the next cell (below the pull-down menu of models) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'Jeff Starn'\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "import scipy.spatial as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gp\n",
    "import mplleaflet\n",
    "from shapely.geometry import box as shape_box\n",
    "from shapely.geometry import Polygon\n",
    "import gdal\n",
    "gdal.UseExceptions()\n",
    "import ogr\n",
    "import osr\n",
    "import pandas as pd\n",
    "from model_specs import *\n",
    "from gen_mod_dict import *\n",
    "pth = 'MFGrid/mfgrid'\n",
    "sys.path.append(pth)\n",
    "import grid as modgrid\n",
    "\n",
    "from ipywidgets import interact, Dropdown\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell doesn't do anything in the notebook. It is the template\n",
    "for turning the notebook into a batch python script. To run in batch mode, download this notebook as a python script and place the entire body of the script in place of the \"pass\" command after the \"try\" statement. Make sure the indent level is the same as \"pass\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in model_dict.items():\n",
    "    md = key\n",
    "    ms = model_dict[md]\n",
    "    print('trying {}'.format(md))\n",
    "    try:\n",
    "        pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model_dict.keys())\n",
    "models.sort()\n",
    "model_area = Dropdown(\n",
    "    options=models,\n",
    "    description='Model:',\n",
    "    background_color='cyan',\n",
    "    border_color='black',\n",
    "    border_width=2)\n",
    "display(model_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run to here to initiate notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First time using this notebook in this session (before restarting the notebook), run the cells up to this point. Then select your model from the dropdown list above. Move your cursor to this cell and use the toolbar menu Cell --> Run All Below.  After the first time, if you want to run another model, select your model and start running from this cell--you don't need to re-run the cells from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = model_area.value\n",
    "ms = model_dict[md]\n",
    "print('The model being processed is {}'.format(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ib_filter = ms['ib_filter']\n",
    "\n",
    "model_ws = os.path.join(proj_dir, ms['ws'])\n",
    "nhd_basin_dir = ms['vpu']\n",
    "rpu = ms['rpu']\n",
    "domain_file = os.path.join(model_ws, ms['df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pathnames for the NHD and Glacial Texture map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDSnapshot', 'Hydrography', 'NHDFlowline.shp')\n",
    "lake_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDSnapshot', 'Hydrography', 'NHDWaterbody.shp')\n",
    "VAA_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDPlusAttributes', 'PlusFlowlineVAA.dbf')\n",
    "fcode_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDSnapshot', 'NHDFCode.dbf')\n",
    "slope_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDPlusAttributes', 'ElevSlope.dbf')\n",
    "\n",
    "#stack_file = os.path.join(geol_dir, 'factor_added_Stack_map.shp')\n",
    "# subsurf_file = os.path.join(geol_dir, 'Subsurface_(Selected_Areas).shp')\n",
    "# surfmat_file = os.path.join(geol_dir, 'Surficial_Materials.shp')\n",
    "# veneer_file = os.path.join(geol_dir, 'Veneer_(Selected_Areas).shp')\n",
    "\n",
    "print(flow_file)\n",
    "print(lake_file)\n",
    "print(VAA_file)\n",
    "print(fcode_file)\n",
    "print(slope_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raster_data(src, method, conversion=1.0):\n",
    "    '''\n",
    "    Takes a raster data source (ESRI grid, GeoTiff, .IMG and many other formats)\n",
    "    and returns a numpy array. Arrangment of pixels is given as input and may \n",
    "    correspond to a MODFLOW grid.\n",
    "    \n",
    "    src : string\n",
    "        complete path to raster data source\n",
    "    method : string\n",
    "        gdal method for interpolation. Choices are:\n",
    "            gdal.GRA_NearestNeighbour \n",
    "                Nearest neighbour (select on one input pixel)\n",
    "            gdal.GRA_Bilinear\n",
    "                Bilinear (2x2 kernel)\n",
    "            gdal.GRA_Cubic\n",
    "                Cubic Convolution Approximation (4x4 kernel)\n",
    "            gdal.GRA_CubicSpline\n",
    "                Cubic B-Spline Approximation (4x4 kernel)\n",
    "            gdal.GRA_Lanczos\n",
    "                Lanczos windowed sinc interpolation (6x6 kernel)\n",
    "            gdal.GRA_Average\n",
    "                Average (computes the average of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Mode\n",
    "                Mode (selects the value which appears most often of all the sampled points)\n",
    "            gdal.GRA_Max\n",
    "                Max (selects maximum of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Min\n",
    "                Min (selects minimum of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Med\n",
    "                Med (selects median of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Q1\n",
    "                Q1 (selects first quartile of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Q3\n",
    "                Q3 (selects third quartile of all non-NODATA contributing pixels)\n",
    "\n",
    "    conversion : float\n",
    "        factor to be applied to raw data values to change units\n",
    "\n",
    "    requires global variables (for now):\n",
    "    NCOL, NROW : number of rows and columns\n",
    "    gt : geotransform list\n",
    "    shapeproj : coordinate reference system of NHDPlus (or other desired projection)\n",
    "    hnoflo : to be used as missing data value (from model_spec.py)\n",
    "\n",
    "    returns:\n",
    "    2D array of raster data source projected onto model grid. \n",
    "    Returns a zero array with the correct shape if the source does not exist.\n",
    "    '''\n",
    "    if os.path.exists(src):\n",
    "        rast = gdal.Open(src)\n",
    "\n",
    "        dest = make_grid(NCOL, NROW, gt, shapeproj)\n",
    "        gdal.ReprojectImage(rast, dest, rast.GetProjection(), shapeproj, method)\n",
    "\n",
    "        grid = dest.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "        grid = grid * conversion\n",
    "\n",
    "        dest = None\n",
    "        rast = None\n",
    "    else:\n",
    "        grid = np.ones((NROW, NCOL)) * hnoflo\n",
    "        print('Data not processed for\\n{}\\n Check that the file exists and path is correct'.format(src))\n",
    "\n",
    "    return grid\n",
    "\n",
    "def process_vector_data(src, attribute):\n",
    "    '''\n",
    "    Takes a vector data source (ESRI shapefile) and returns a numpy array.\n",
    "    Arrangment of pixels is given as input and may correspond to a MODFLOW grid.\n",
    "\n",
    "    src : complete path to vector data source\n",
    "    attribute : field in data table to assign to rasterized pixels\n",
    "    \n",
    "    requires global variables:\n",
    "    NCOL, NROW : number of rows and columns\n",
    "    gt : geotransform list\n",
    "    shapeproj : coordinate reference system of NHDPlus\n",
    "    hnoflo : to be used as missing data value (from model_spec.py)\n",
    "    \n",
    "    returns:\n",
    "    2D array of vector data source projected onto model grid.\n",
    "    Returns a zero array with the correct shape if the source does not exist.\n",
    "    '''\n",
    "    if os.path.exists(src):\n",
    "\n",
    "        datasource = ogr.Open(src)\n",
    "        layer = datasource.GetLayer()\n",
    "\n",
    "        src = make_grid(NCOL, NROW, gt, shapeproj, 0)\n",
    "        args = 'ATTRIBUTE={}'.format(attribute)\n",
    "        gdal.RasterizeLayer(src, [1], layer, options = [args])\n",
    "\n",
    "        grid = src.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "        src = None\n",
    "        dst = None        \n",
    "    else:\n",
    "        grid = np.ones((NROW, NCOL)) * hnoflo\n",
    "        print('Data not processed for\\n{}\\n Check that the file exists and path is correct'.format(src))\n",
    "\n",
    "    return grid\n",
    "\n",
    "def make_raster(dst_file, data, NCOL, NROW, gt, proj, nodata):\n",
    "    '''\n",
    "    Writes numpy array to a GeoTiff file.\n",
    "    \n",
    "    dst_file : name of file to write\n",
    "    data : 2D numpy array\n",
    "    NCOL, NROW : number of rows and columns. These may coincide with a MODFLOW grid.\n",
    "    gt : 6-element geotransform list [C, A, B, F, E, D]. Gives the coordinates of one pixel\n",
    "        (the upper left pixel). If there is no rotation, B=D=0. If cells are square, A=-E.   \n",
    "        Letter designations come from the original documentation.\n",
    "        \n",
    "        C = x coordinate in map units of the upper left corner of the upper left pixel\n",
    "        A = distance from C along x axis to upper right pixel corner of the upper left pixel\n",
    "        B = distance from C along x axis to lower left pixel corner of the upper left pixel,\n",
    "        F = y coordinate in map units of the upper left corner of the upper left pixel\n",
    "        E = distance from C along y axis to lower left pixel corner of the upper left pixel\n",
    "        D = distance from C along y axis to upper right pixel corner of the upper left pixel\n",
    "        \n",
    "    proj : projection of the GeoTiff\n",
    "    nodata : value to use as missing data in the GeoTiff\n",
    "    '''\n",
    "    import gdal\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst = driver.Create(dst_file, NCOL, NROW, 1, gdal.GDT_Float32)\n",
    "    dst.SetGeoTransform(gt)\n",
    "    dst.SetProjection(proj)\n",
    "    band = dst.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata)\n",
    "    band.WriteArray(data)\n",
    "    dst = None\n",
    "\n",
    "def make_grid(NCOL, NROW, gt, proj, nodata=hnoflo):\n",
    "    '''\n",
    "    Creates a blank raster image in memory.\n",
    "        \n",
    "    NCOL, NROW : number of rows and columns. These may coincide with a MODFLOW grid.\n",
    "    gt : 6-element geotransform list [C, A, B, F, E, D]. Gives the coordinates of one pixel\n",
    "        (the upper left pixel). If there is no rotation, B=D=0. If cells are square, A=-E.   \n",
    "        Letter designations come from the original documentation.\n",
    "        \n",
    "        C = x coordinate in map units of the upper left corner of the upper left pixel\n",
    "        A = distance from C along x axis to upper right pixel corner of the upper left pixel\n",
    "        B = distance from C along x axis to lower left pixel corner of the upper left pixel,\n",
    "        F = y coordinate in map units of the upper left corner of the upper left pixel\n",
    "        E = distance from C along y axis to lower left pixel corner of the upper left pixel\n",
    "        D = distance from C along y axis to upper right pixel corner of the upper left pixel\n",
    "        \n",
    "    proj : projection of the GeoTiff\n",
    "    nodata : value to use as missing data in the GeoTiff\n",
    "    '''\n",
    "    import gdal\n",
    "    mem_drv = gdal.GetDriverByName('MEM')\n",
    "    grid_ras = mem_drv.Create('', NCOL, NROW, 1, gdal.GDT_Float32)\n",
    "    grid_ras.SetGeoTransform(gt)\n",
    "    grid_ras.SetProjection(shapeproj)\n",
    "    band = grid_ras.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata)\n",
    "    array = np.zeros((NROW,NCOL))\n",
    "    band.WriteArray(array)\n",
    "    return grid_ras\n",
    "\n",
    "def process_mohp_data(tif_files):\n",
    "    '''\n",
    "    Loops a list of MOHP tif files. The rest of the algorithm is similar to the function\n",
    "    \"process_raster_data\" except that a transformation from the ESRI WKT format to a generic\n",
    "    format is needed. When MOHP data source is finalized, this function can be modified\n",
    "    to work with the final format.\n",
    "    \n",
    "    src : complete path to raster data source\n",
    "    method : gdal method for interpolation\n",
    "    conversion : factor to be applied to raw data values to change units\n",
    "\n",
    "    requires global variables (for now):\n",
    "    NCOL, NROW : number of rows and columns\n",
    "    gt : geotransform list\n",
    "    shapeproj : coordinate reference system of NHDPlus (or other desired projection)\n",
    "    hnoflo : to be used as missing data value (from model_spec.py)\n",
    "\n",
    "    returns:\n",
    "    2D array of raster data source projected onto model grid. Each column contains\n",
    "    a different stream order MOHP. Each row corresponds to a model cell. \n",
    "    Number of rows is NCOL x NCOL. Number of columns is number of stream orders present.\n",
    "    '''\n",
    "    import gdal\n",
    "    gdal.UseExceptions()\n",
    "    import ogr\n",
    "    import osr\n",
    "    \n",
    "    arr = np.zeros((NCOL * NROW, len(tif_files)))\n",
    "    if tif_files != []:\n",
    "        for col, src in enumerate(tif_files):\n",
    "            hp = gdal.Open(src)\n",
    "\n",
    "            dest = make_grid(NCOL, NROW, gt, shapeproj)\n",
    "\n",
    "            srs = osr.SpatialReference()\n",
    "            srs.ImportFromWkt(hp.GetProjection())\n",
    "            srs.MorphFromESRI()\n",
    "            hp_prj = srs.ExportToWkt()\n",
    "            hp.SetProjection(hp_prj)\n",
    "\n",
    "            gdal.ReprojectImage(hp, dest, hp.GetProjection(), shapeproj, gdal.GRA_NearestNeighbour)\n",
    "\n",
    "            hp_grd = dest.GetRasterBand(1).ReadAsArray()\n",
    "            hp_grd = hp_grd / 10000.\n",
    "\n",
    "            dst = None\n",
    "            hp = None\n",
    "            \n",
    "            arr[:, col] = hp_grd.ravel()\n",
    "    return arr\n",
    "\n",
    "def make_clockwise(coords):\n",
    "    '''\n",
    "    Function to determine direction of vertices of a polygon (clockwise or CCW).\n",
    "    Probably not needed, but here just in case. \n",
    "    \n",
    "    coords : array with dim (n, 2)\n",
    "            n is number of vertices in the polygon. The last vertex is the same \n",
    "            as the first to close the polygon. The first column is x and the second is y.\n",
    "    '''\n",
    "    # if the points are counterclockwise, reverse them\n",
    "    x1 = coords[:-1, 0]\n",
    "    x2 = coords[1:, 0]\n",
    "    y1 = coords[:-1, 1]\n",
    "    y2 = coords[1:, 1]\n",
    "    ccw = np.sum((x2 - x1) * (y2 + y1)) < 0\n",
    "    if ccw:\n",
    "        coords = np.flipud(coords)\n",
    "        print('yup, coordinates are ccw')\n",
    "        print(\"let's change them to CW\")\n",
    "    return coords\n",
    "\n",
    "# test data for make_clockwise\n",
    "\n",
    "# print('clockwise')\n",
    "# x = np.array([1, 1, 2, 2, 1])\n",
    "# y = np.array([1, 2, 2, 1, 1])\n",
    "# coords = np.array(zip(x, y))\n",
    "# c = make_clockwise(coords)\n",
    "# print( c)\n",
    "# print('\\n')\n",
    "# print('CCW')\n",
    "# x = np.array([1, 2, 2, 1, 1])\n",
    "# y = np.array([1, 1, 2, 2, 1])\n",
    "# coords = np.array(zip(x, y))\n",
    "# c = make_clockwise(coords)\n",
    "# print( c)\n",
    "\n",
    "import pysal as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from shutil import copyfile\n",
    "\n",
    "def dbf2df(dbf_path, index=None, cols=False, incl_index=False):\n",
    "    '''\n",
    "    Read a dbf file as a pandas.DataFrame, optionally selecting the index\n",
    "    variable and which columns are to be loaded.\n",
    "\n",
    "    __author__  = \"Dani Arribas-Bel <darribas@asu.edu> \"\n",
    "    ...\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    dbf_path    : str\n",
    "                  Path to the DBF file to be read\n",
    "    index       : str\n",
    "                  Name of the column to be used as the index of the DataFrame\n",
    "    cols        : list\n",
    "                  List with the names of the columns to be read into the\n",
    "                  DataFrame. Defaults to False, which reads the whole dbf\n",
    "    incl_index  : Boolean\n",
    "                  If True index is included in the DataFrame as a\n",
    "                  column too. Defaults to False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df          : DataFrame\n",
    "                  pandas.DataFrame object created\n",
    "    '''\n",
    "    db = ps.open(dbf_path)\n",
    "    if cols:\n",
    "        if incl_index:\n",
    "            cols.append(index)\n",
    "        vars_to_read = cols\n",
    "    else:\n",
    "        vars_to_read = db.header\n",
    "    data = dict([(var, db.by_col(var)) for var in vars_to_read])\n",
    "    if index:\n",
    "        index = db.by_col(index)\n",
    "        db.close()\n",
    "        return pd.DataFrame(data, index=index)\n",
    "    else:\n",
    "        db.close()\n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate model grid coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the steps for creating and populating the MODFLOW grid in GIS space is as follows:\n",
    "\n",
    "* Read the domain shapefile and store its projection. The shapefile containing outline of model domain must have an attribute called \"ibound.\"\n",
    "* Make the convex hull of the model domain and extract the coordinates of the hull.\n",
    "* Iterate around the sides of the convex hull, rotating it so that each iteration the next side is parallel to the x axis.\n",
    "* Make a bounding box around the rotated hull and store its area.\n",
    "* The angle of rotation that corresponds to the bounding box with the smallest area is the rotation that produces the smallest number of inactive cells.\n",
    "* Make a geotransform list so that any arbitrary data can be projected on to the model grid in GIS space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_diss = gp.read_file(domain_file)\n",
    "prj = domain_diss.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the NED grid from NHDPlus to get the projection. NHDPlus lines are in geographic coordinates rather than a projected coordinate reference system. Extract the projection as WKT, convert it to Proj.4 format, then parse that into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "ned = gdal.Open(src)\n",
    "shapeproj = ned.GetProjection()\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromWkt(shapeproj)\n",
    "prj4 = (srs.ExportToProj4()).split('+')\n",
    "prj4 = dict([item.split('=') for item in prj4 if len(item.split('=')) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_diss.to_crs(prj4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_diss.ibound = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_diss.plot()\n",
    "domain_diss.to_file(os.path.join(model_ws, 'domain_outline.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following lines to generate a leaflet map of the model domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = domain_diss.plot()\n",
    "#mplleaflet.display(fig=ax.figure, crs=prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find convex hull and extract the coordinates of its vertices. (Add the function to change to clockwise here if needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull = domain_diss.convex_hull\n",
    "coords2 = np.array(hull[0].exterior.coords[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the domain (dc) and angle (da) of each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc2 = np.diff(coords2, axis=0)\n",
    "da2 = np.arctan2(dc2[:,1], dc2[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate to each side of the convex hull and calculate the area of each bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vert = da2.shape[0]\n",
    "area2 = np.zeros((num_vert))\n",
    "for i in range(num_vert):\n",
    "    hull_rot = hull.rotate(-da2[i], origin=tuple(coords2[i, :]), use_radians=True)\n",
    "    minx, miny, maxx, maxy = hull_rot.bounds.iloc[0]\n",
    "    area2[i] = (maxx - minx) * (maxy - miny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a rectangular polygon and geoseries corresponding to the minimum area bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii2 = np.argmin(area2)\n",
    "hull_rot = hull.rotate(-da2[ii2], origin=tuple(coords2[ii2, :]), use_radians=True)\n",
    "# minx, miny, maxx, maxy = hull_rot.bounds.iloc[0]\n",
    "# temp_poly = shape_box(minx, miny, maxx, maxy, ccw=False)\n",
    "# temp_gdf = gp.GeoSeries(temp_poly)\n",
    "temp_gdf = hull_rot.envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate the bounding box back to its original coordinate framework and set its projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = temp_gdf.rotate(da2[ii2], origin=tuple(coords2[ii2, :]), use_radians=True)\n",
    "box.crs = prj4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box.to_file(os.path.join(model_ws, 'clip_box.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the geometry for use later in clipping NHD flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = box.geometry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a visualing pleasing origin and use it as the model origin. \n",
    "\n",
    "Extract the coordinates of the box; each line is an (x, y) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.array(box[0].exterior.coords[:])\n",
    "pts = make_clockwise(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and last vertices of a polygon are identical, so strip off the last vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pts = pts[:-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the line containing the coordinates of the apex (ymax) of the hull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax = np.argmax(pts[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap (roll) the lines of the array around so that the apex is at the top of the array (first line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pts = np.roll(pts, -ymax, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the first point (ymax) back to the last position to complete the polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pts = np.vstack((pts, pts[0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the domain (dc) and angle (da) of each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = np.diff(pts, axis=0)\n",
    "da = np.arctan2(dc[:, 1], dc[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine shape of bounding box to determine row and column directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dx, dy = np.amax(dc, axis=0)\n",
    "# is_tall = dx < dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the geotransform list to be used for creating raster images later. See the function \"make_raster\" for an explanation. Theta is not used to make rasters; it's used to rotate NHD flowlines.\n",
    "\n",
    "The orientation of the top side of the model grid is determined by the angle from (x, ymax) to the next vertex to its right in a clockwise direction:\n",
    "\n",
    "* If the angle is < -45 degrees (-pi/4), the top side is to the left of (x, ymax) and the origin is (x, ymax - 1)\n",
    "    \n",
    "* If the angle is >= -45 degrees (-pi/4), the top side is to the right of (x, ymax) and the origin is (x, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_lengths = np.hypot(dc[:, 0], dc[:, 1])\n",
    "\n",
    "if da[ymax] < (np.pi / -4):\n",
    "    or_cor = ymax - 1\n",
    "else:\n",
    "    or_cor = ymax\n",
    "\n",
    "theta = da[or_cor]\n",
    "origin = pts[or_cor, :]\n",
    "x_len = side_lengths[or_cor]\n",
    "y_len = side_lengths[or_cor - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of rows and columns for the L given in model_spec.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NROW = np.int(np.ceil(y_len / L))\n",
    "NCOL = np.int(np.ceil(x_len / L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (NROW, NCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the number of rows and columns with those from an existing model if one is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ms.keys()\n",
    "if 'NROW' in keys:\n",
    "    NROW = ms['NROW']\n",
    "    NCOL = ms['NCOL']\n",
    "    LX = x_len / NCOL\n",
    "    LY = y_len / NROW\n",
    "else:\n",
    "    LX = L\n",
    "    LY = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = LX * np.cos(theta)\n",
    "B = LY * np.sin(theta)\n",
    "D = LX * np.sin(theta)\n",
    "E = LY * -np.cos(theta)\n",
    "\n",
    "gt = [origin[0], A, B, origin[1], D, E]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a file of information that summarizes all of the above stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_spec_file = os.path.join(model_ws, 'grid_spec.txt')\n",
    "with open(grid_spec_file, 'w') as f:\n",
    "    line = 'Source locations\\n'\n",
    "    f.write(line)\n",
    "    for key, val in ms.items():\n",
    "        f.write('  {} : {}\\n'.format(key, val))\n",
    "    line = '\\nUpper left corner x and y projected coordinates\\n'\n",
    "    f.write(line)\n",
    "    line = '{} {}\\n\\n'.format(origin[0], origin[1])\n",
    "    f.write(line)\n",
    "    line = 'Rotation about upper left corner in radians and degrees from positive x axis\\n'\n",
    "    f.write(line)\n",
    "    line = '{} {}\\n\\n'.format(theta, theta * 180 / np.pi)\n",
    "    f.write(line)\n",
    "    line = 'Grid corner projected coordinates\\n'\n",
    "    f.write(line)\n",
    "    for row in pts:\n",
    "        f.write('{:0.2f} {:0.2f}'.format(*row))\n",
    "    line = '\\nCoordinate reference system\\n'\n",
    "    f.write(line)\n",
    "    for key, val in prj4.items():\n",
    "        f.write('    {} : {}\\n'.format(key, val))\n",
    "    line = '\\n\\nGeotransformation block\\n'\n",
    "    f.write(line)\n",
    "    line = '{} {} {} {} {} {}'.format(*gt)\n",
    "    f.write(line)\n",
    "    line = '\\n\\nGrid block\\n'\n",
    "    f.write(line)\n",
    "    line = 'NROW {} NCOL {}'.format(NROW, NCOL)\n",
    "    f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process river flowlines and lake polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read HD flowlines and associated tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowlines = gp.read_file(flow_file)\n",
    "VAA_tab = dbf2df(VAA_file, cols=['ComID', 'StreamOrde'])\n",
    "slope_tab = dbf2df(slope_file, cols=['COMID', 'MAXELEVSMO', 'MINELEVSMO'])\n",
    "fcode_tab = dbf2df(fcode_file, cols=['FCode', 'Hydrograph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract projection information, which for NHDPlus flowlines will be geographic (lat long) coordinates. Project grid box outline to the NHD (geographic) coordinates. Clip the NHD to the box and project it back to the projected coordinate system (probably Albers). Note: the crs from NHD is given below as epsg code 4269.  This is the same as a plain NAD83 crs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlong_prj = flowlines.crs\n",
    "latlong_box = box.to_crs(crs=latlong_prj)\n",
    "latlong_clip = latlong_box.geometry[0]\n",
    "lines = flowlines['geometry'].intersection(latlong_clip)\n",
    "lines = lines.to_crs(crs=prj4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add fields from the NHD back to the clipped lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.DataFrame(lines, columns=['geometry'])\n",
    "lines['COMID'] = flowlines.COMID\n",
    "lines['FCODE'] = flowlines.FCODE\n",
    "lines['REACHCODE'] = flowlines.REACHCODE\n",
    "lines['LENGTHKM'] = flowlines.LENGTHKM\n",
    "f = lambda x: x[:8]\n",
    "lines['HUC_8'] = lines.REACHCODE.map(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge selected fields from the associated tables into the clipped lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.merge(VAA_tab, how='inner', left_on='COMID', right_on='ComID')\n",
    "lines = lines.merge(slope_tab, how='inner', left_on='COMID', right_on='COMID')\n",
    "lines = lines.merge(fcode_tab, how='inner', left_on='FCODE', right_on='FCode')\n",
    "\n",
    "lines['maxft'] = lines.MAXELEVSMO / 100.\n",
    "lines['minft'] = lines.MINELEVSMO / 100.\n",
    "lines['StreamOrde'] = lines.StreamOrde\n",
    "\n",
    "lines.drop(['FCode', 'ComID'], axis=1, inplace=True)\n",
    "# lines = lines[lines.Hydrograph != 'Intermittent']\n",
    "lines['intermit'] = lines.Hydrograph == 'Intermittent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a GeoDataFrame from the lines and get rid of lines outside the box (model) area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = gp.GeoDataFrame(lines, crs=prj4, geometry=lines.geometry)\n",
    "lines = lines[-lines.geometry.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a HUC-8 is provided in domain_diss, do the above operations again, this time for the basin outline rather than the model box.  It's faster to perform the operations in two steps like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_clip = domain_diss.geometry[0]\n",
    "lines_clip = lines['geometry'].intersection(prj_clip)\n",
    "\n",
    "lines_clip = gp.GeoDataFrame(lines_clip, crs=prj4, geometry=lines_clip.geometry)\n",
    "lines_clip['COMID'] = lines.COMID\n",
    "lines_clip['FCODE'] = lines.FCODE\n",
    "lines_clip['REACHCODE'] = lines.REACHCODE\n",
    "lines_clip['HUC_8'] = lines.HUC_8\n",
    "\n",
    "lines_clip['maxft'] = lines.maxft\n",
    "lines_clip['minft'] = lines.minft\n",
    "lines_clip['StreamOrde'] = lines.StreamOrde\n",
    "lines_clip['LENGTHKM'] = lines.LENGTHKM\n",
    "lines_clip['intermit'] = np.int32(lines.intermit)\n",
    "\n",
    "lines_clip.drop(0, axis=1, inplace=True)\n",
    "lines_clip = lines_clip[-lines_clip.geometry.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clipped flowlines to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_clip.to_file(os.path.join(model_ws, 'NHD_clip.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate values (offset and rotation) to transform flowlines to model grid reference system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xoff = origin[0]\n",
    "y_len_grid = NROW * LY\n",
    "yoff = origin[1] - y_len_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate and translate flowlines to model grid reference system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_rot = lines_clip.rotate(-theta, origin=tuple(origin), use_radians=True).translate(xoff=-xoff, yoff=-yoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create drain/river package input dictionary of lists (for FloPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First create a ModflowGrid object.  This class is available in FloPy and makes doing the intersections of flowlines with model grid easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NLAY = 1\n",
    "mfg = modgrid.ModflowGrid(NLAY, NROW, NCOL, LX, LY, 0)\n",
    "\n",
    "NPER =  1 \n",
    "drn_dict = {}\n",
    "perlist = []\n",
    "lineset = range(lines.shape[0])\n",
    "riv_loc = np.zeros((NROW,NCOL), np.int)\n",
    "riv_stg = np.zeros((NROW,NCOL), np.float)\n",
    "order_dict = {}\n",
    "per_order_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loop through each stress period. Each set of drains/rivers is a value in a dictionary whose key is the stress period.\n",
    "* Two parallel lists are used: lines_clip (in geographic space) and lines.rot (in model space)\n",
    " * intersections are done in model space so that lengths of flowline in model cell can be calculated\n",
    " * attributes aren't merged into lines_rot, so attributes have to be taken from lines_clip\n",
    " * the simple rotation and translation seems to preserve the order in both lists, but this could be a potential issue\n",
    "* mfg.intersection returns\n",
    " * nodes : tuples of (row, column) that are intersected by a flowline reach\n",
    " * lengths : scalar lengths of flowline in each model cell corresponding to the above tuple\n",
    "* Each reach has a starting and ending elevation from NHDPlus. Interpolate these to the midpoint of each intersected segment in each cell. If starting and ending elevations are the same, return zero-gradient stream segments.\n",
    "* Loop through each set of nodes for each reach, adding each one to the list of drain/river cells for the current stress period. \n",
    "* Some reaches have (probably erroneously) have begining and ending stages = 0. These are ignored, but this could be an issue for coastal areas with discharge to estuaries (stage = 0).\n",
    "* Maintain a second list with information on reach ID, original length, and stream order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for period in range(NPER):\n",
    "    for rindex, reach in lines_clip.iterrows():\n",
    "        try:\n",
    "            data = np.array(lines_rot.geometry[rindex].coords[:])\n",
    "            nodes, lengths = mfg.intersection(data, 'line')\n",
    "\n",
    "    #         interpolate the begin and end segment elevation to the midpoint of the feature in each cell\n",
    "            cumlen = np.cumsum(lengths)\n",
    "            midpoint = cumlen - np.asarray(lengths) / 2\n",
    "            rise = reach.maxft - reach.minft\n",
    "            grad = rise / cumlen[-1]\n",
    "            stage = reach.maxft - midpoint * grad\n",
    "            if reach.maxft == reach.minft:\n",
    "                stage = np.ones_like(stage) * reach.minft\n",
    "    #         loop through the cells intersected by the feature\n",
    "            for nindex, (row, col) in enumerate(nodes):\n",
    "                riv_loc[row, col] = 1\n",
    "                temp = [0, row, col, stage[nindex], lengths[nindex]]\n",
    "                temp2 = [0, row, col, reach.StreamOrde, reach.REACHCODE, reach.LENGTHKM, reach.intermit]        \n",
    "                if stage[nindex] != 0:\n",
    "                    perlist.append(temp)\n",
    "                    per_order_list.append(temp2)\n",
    "        except (NotImplementedError, IndexError):\n",
    "            pass\n",
    "    drn_dict[period] = perlist\n",
    "    order_dict[period] = per_order_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the drain/river list and put those values in an array using (row, column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# riv_data = drn_dict\n",
    "riv_len = np.ones((NROW, NCOL), np.float) * hnoflo\n",
    "riv_stg = np.ones((NROW, NCOL), np.float) * hnoflo\n",
    "\n",
    "riv_data = drn_dict[0]\n",
    "for [l, r, c, s, cond] in riv_data:\n",
    "    row = int(r)\n",
    "    col = int(c)\n",
    "    stg = float(s)\n",
    "    stk = float(cond)\n",
    "    riv_len[row, col] = stk\n",
    "    riv_stg[row, col] = stg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above but with the lists of stream order and reach ID. lengthkm is the original reach length from NHDPlus, not the intersected length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riv_data = order_dict\n",
    "riv_ord = np.ones((NROW ,NCOL), np.int) * hnoflo\n",
    "riv_comid = np.zeros((NROW, NCOL)).astype(str) \n",
    "riv_reachlen = np.ones((NROW, NCOL), np.int) * hnoflo\n",
    "riv_intermit = np.zeros((NROW, NCOL)).astype(bool) \n",
    "\n",
    "riv_data = order_dict[0]\n",
    "for [l, r, c, order, comid, lengthkm, intermit] in riv_data:\n",
    "    row = int(r) \n",
    "    col = int(c) \n",
    "    order = int(order)\n",
    "    reachlen = float(lengthkm)\n",
    "    riv_ord[row, col] = order\n",
    "    riv_comid[row, col] = comid\n",
    "    riv_reachlen[row, col] = reachlen\n",
    "    riv_intermit[row, col] = intermit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an attribute to the lake file for rasterizing later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_lakes = gp.read_file(lake_file)\n",
    "lakes = gp.read_file(lake_file)\n",
    "lakes['is_lake'] = 1\n",
    "lakes.to_crs(prj4, inplace=True)\n",
    "lakes.to_file(os.path.join(model_ws, 'lakes.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project data sources onto modflow grid, store the data in a file called model_grid.csv, and write Geotiff images of all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a blank dataframe and add 2D node numbers (NROW * NCOL) as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid = pd.DataFrame()\n",
    "model_grid['node_num'] = np.arange(NROW * NCOL)\n",
    "model_grid.set_index('node_num', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and read the NED grid for the current model area and store its projection. All other data sources will be projected into this projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "ned = gdal.Open(src)\n",
    "shapeproj = ned.GetProjection()\n",
    "ned = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add vector data sources to model_grid dataframe.\n",
    "* Better: put parameters in a nested list and loop through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is from an inset or other more detailed model, read the ibound array based on the corresponding General Model for consistency. This is coded in the second block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'ibound_src' in ms.keys():\n",
    "#     ib_src = model_dict[ms['ibound_src']]['ws']\n",
    "#     ib_src = os.path.join(proj_dir, ib_src)\n",
    "#     ib_file = model_dict[ms['ibound_src']]['df']\n",
    "#     ib_shp = os.path.join(ib_src, ib_file)\n",
    "#     grid = process_vector_data(ib_shp, 'ibound')\n",
    "#     model_grid['ibound'] = grid.ravel()   \n",
    "# else:\n",
    "src = os.path.join(model_ws, 'domain_outline.shp')\n",
    "grid = process_vector_data(src, 'ibound')\n",
    "model_grid['ibound'] = grid.ravel()\n",
    "\n",
    "#src = os.path.join(model_ws, gage_file)\n",
    "#grid = process_vector_data(src, 'GAGE_ID')\n",
    "#model_grid['gage_id'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(model_ws, 'NHD_clip.shp')\n",
    "grid = process_vector_data(src, 'StreamOrde')\n",
    "model_grid['stream_order'] = grid.ravel()\n",
    "\n",
    "gess_geology_file = 'GESS_poly.gdb'\n",
    "src = os.path.join(qa_dir, gess_geology_file)\n",
    "grid = process_vector_data(src, 'CrseStratSed')\n",
    "model_grid['gess_poly'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(model_ws, 'lakes.shp')\n",
    "grid = process_vector_data(src, 'is_lake')\n",
    "model_grid['lake'] = grid.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add raster data sources to model_grid dataframe.\n",
    "* Better: put parameters in a nested list and loop through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, 0.01)\n",
    "model_grid['ned'] = grid.ravel() # units of meters\n",
    "\n",
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 0.01)\n",
    "model_grid['ned_mean'] = grid.ravel() # units of meters\n",
    "\n",
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NHDPlusCatchment', 'cat')\n",
    "grid = process_raster_data(src, gdal.GRA_NearestNeighbour, 1)\n",
    "model_grid['catchment'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(soller_surf_thick_dir, 'sim3392_sheet1_driftthickness.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, ft2m) # convert feet to meters\n",
    "model_grid['soller_thk'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(soller_bedrock_topo_dir, 'sim3392_sheet2_bedrocktopo.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, ft2m) # convert feet to meters\n",
    "model_grid['bedrock_el'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(nlcd_dir, 'nlcd_2011_landcover_2011_edition_2014_10_10.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Mode, 1)\n",
    "model_grid['nlcd'] = grid.ravel()\n",
    "\n",
    "\n",
    "#Two sources for recharge values\n",
    "\n",
    "#Used for recharge, Reitz published values\n",
    "src = os.path.join(recharge_reitz, 'RC_eff_2013.tif') #Reitz, published, values in this file are m/year, no need to convert\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, 1.0)\n",
    "model_grid['rch_eff_m_Reitz_2013'] = grid.ravel()\n",
    "\n",
    "#The Wolock recharge below is included as an alternative to Reitz for comparison purposes; user will have to specify in JN3\n",
    "src = os.path.join(alt_recharge_wolock, 'rech48grd') #Wolock, published, values in this file are mm/year, need to convert to m\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, 0.001)\n",
    "model_grid['rch_m_Wolock'] = grid.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process MOHP data. Each region could have a different number of stream-order-based MOHP data sources, so this processing has to be handled differently from regular raster sources.  One complication is that the projection information has to be translated from ESRI's custom version of the WKT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    srcdir = mohp_dir\n",
    "    rpu_file = 'pctdistR{}'.format(int(rpu[3:5]))\n",
    "    tif_files = []\n",
    "\n",
    "# search MOHP first-generation directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for f in files:\n",
    "            if f.endswith('tif'):\n",
    "                if rpu_file in f:\n",
    "                    src_pth = os.path.join(root, f)\n",
    "                    tif_files.append(src_pth)\n",
    "                \n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'HP_{}'.format(key[9:])\n",
    "        model_grid[key] = arr[:, col]\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  \n",
    "    srcdir = mohp2_dir\n",
    "    tif_files = []\n",
    "\n",
    "# search MOHP second-generation directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for f in files:\n",
    "            if f.endswith('tif'):\n",
    "                src_pth = os.path.join(root, f)\n",
    "                tif_files.append(src_pth)\n",
    "\n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'HP2_{}'.format(key.split('_')[0])\n",
    "        model_grid[key] = arr[:, col]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    srcdir = thies2_dir\n",
    "    tif_files = []\n",
    "\n",
    "# search MOHP-Thiessen polygon directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for f in files:\n",
    "            if f.endswith('tif'):\n",
    "                src_pth = os.path.join(root, f)\n",
    "                tif_files.append(src_pth)\n",
    "\n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'HPTH_{}'.format(key.split('_')[0])\n",
    "        model_grid[key] = arr[:, col]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    srcdir = pour_dir\n",
    "    rpu_file = 'or{}'.format(int(rpu[3:5]))\n",
    "    grid_files = []\n",
    "\n",
    "# search MOHP pour-point-elevation directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for d in dirs:\n",
    "            if d[:2] == 'or':\n",
    "                src_pth = os.path.join(root, d)\n",
    "                grid_files.append(src_pth)\n",
    "\n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'PP_{}'.format(key[2:])\n",
    "        model_grid[key] = arr[:, col]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the ibound array to get rid of disconnected cells.  This is a trial-and-error process. The filter is selected in model_spec.py. One of these filters can be used if there are isolated nodes appearing because of the discretization. The filter is specified in the model dictionary in model_spec.py.\n",
    "\n",
    "use nd.image.measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ib = model_grid.ibound.reshape(NROW, NCOL)\n",
    "# if ib_filter == 0:\n",
    "#     pass\n",
    "# elif ib_filter == 1:\n",
    "#     filtr = np.array([[0,1,0], [1,4,1], [0,1,0]])\n",
    "#     ib = nd.correlate(ib, filtr, mode='constant', cval=0) > 5\n",
    "# elif ib_filter == 2:\n",
    "#     # alternate filter 11/5/2014\n",
    "#     filtr = np.array([[0,0,1,0,0], [0,0,4,0,0], [1,4,16,4,1], [0,0,4,0,0], [0,0,1,0,0]])\n",
    "#     ib = nd.correlate(ib, filtr, mode='constant', cval=0) > 29\n",
    "# else: \n",
    "#     pass\n",
    "\n",
    "# ib = ib.astype(int)\n",
    "# model_grid['ibound'] = ib.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An improved filter than that above.\n",
    "ib = model_grid.ibound.values.reshape(NROW, NCOL)\n",
    "'''\n",
    "Removes isolated active cells from the IBOUND array.\n",
    "Args: ibound array (nlay,nrow,ncol)\n",
    "\n",
    "from Wesley Zell\n",
    "'''\n",
    "\n",
    "# Distinguish disconnected clusters of active cells in the IBOUND array.\n",
    "# 0 is considered background; in MODLFOW, active cells are != 0.\n",
    "ib[ib != 0] = 1\n",
    "array_of_cluster_idx, num = nd.measurements.label(ib)\n",
    "\n",
    "# Identify the cluster with the most active cells; this is the main active area\n",
    "areas = nd.measurements.sum(ib, array_of_cluster_idx,\\\n",
    "                         index=np.arange(array_of_cluster_idx.max() + 1))\n",
    "max_cluster_idx = np.argmax(areas)\n",
    "\n",
    "# Inactivate all cells that belong to secondary clusters (e.g., islands)\n",
    "ib[array_of_cluster_idx != max_cluster_idx] = 0\n",
    "ib = ib.astype(int)\n",
    "model_grid['ibound'] = ib.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter ibound to make a grid of edge cells. Intersect this with the lake grid. Cell that are on the edge of the model and are in lakes could be treated as GHB cells.  This is not implemented yet, but the code would be very similar to the code above for drain/river cells. FloPy GHB requires a dictionary of lists similar to river/drain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtr = np.ones((3,3))\n",
    "not_solid = nd.correlate(ib, filtr, mode='constant', cval=0) < 9\n",
    "\n",
    "edge = (ib + not_solid) == 2\n",
    "edge = edge.astype(int)\n",
    "model_grid['edge'] = edge.ravel()\n",
    "\n",
    "lakes_arr = model_grid.lake.values.reshape(NROW, NCOL)\n",
    "lake_blur = nd.correlate(lakes_arr, filtr, mode='constant', cval=0) > 0\n",
    "\n",
    "ghb = (edge + lake_blur) == 2\n",
    "ghb = ghb.astype(int)\n",
    "model_grid['ghb'] = ghb.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add drain/river information to model_grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid['stage'] = riv_stg.ravel()\n",
    "model_grid['segment_len'] = riv_len.ravel()\n",
    "model_grid['order'] = riv_ord.ravel()\n",
    "model_grid['reachcode'] = riv_comid.ravel()\n",
    "model_grid['reach_intermit'] = riv_intermit.ravel()\n",
    "model_grid['reach_len'] = riv_reachlen.ravel()\n",
    "model_grid['reach_int'] = pd.Categorical(model_grid.reachcode).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NHDPlus has a small number of MAXELEVSMO and MINELEVSMO in (mostly?) first order streams that are set to a missing value code -9980. Converted to meters this is -99.80. Replace all such values with the NED value minus 1 (meter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid['stage'] = np.where(model_grid.stage == -99.98, model_grid.ned - 1., model_grid.stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace hnoflo values with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid[model_grid == hnoflo] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace grid-cell-mean NED elevations with interpolated stream stage, but only in model cells that contain a stream. The resulting data is called 'top'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid['top'] = model_grid.ned_mean\n",
    "index = model_grid.stage.notnull()\n",
    "model_grid.loc[index, 'top'] = model_grid.stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add layer, row, column to model_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid['lay'] = 0\n",
    "row, col = np.mgrid[0:NROW:1, 0:NCOL:1]\n",
    "\n",
    "model_grid['row'] = row.ravel()\n",
    "model_grid['col'] = col.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add observation type to cells.  \"hydro\" if a perennial stream, \"topo\" everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_active = model_grid.ibound != 0\n",
    "is_intermit = model_grid.reach_intermit\n",
    "hydro_index = model_grid.stage.notnull()\n",
    "topo_index = model_grid.stage.isnull()\n",
    "\n",
    "is_hydro_obs = is_active & ~is_intermit & hydro_index\n",
    "is_topo_obs = is_active & ~is_hydro_obs\n",
    "\n",
    "model_grid.loc[is_hydro_obs, 'obs_type'] = 'hydro'\n",
    "model_grid.loc[is_topo_obs, 'obs_type'] = 'topo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add distance to nearest stream cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delr = np.ones((NCOL)) * L\n",
    "delc = np.ones((NROW)) * L\n",
    "grid_len_y = L * NROW\n",
    "x = np.cumsum(delr) - delr / 2 \n",
    "y = grid_len_y - (np.cumsum(delc) - delc / 2 )\n",
    "xc, yc = np.meshgrid(x, y)\n",
    "model_grid['xc'] = xc.ravel()\n",
    "model_grid['yc'] = yc.ravel()\n",
    "\n",
    "stream_cells = model_grid.loc[is_hydro_obs, ['xc', 'yc']]\n",
    "topo_cells = model_grid.loc[is_topo_obs, ['xc', 'yc']]\n",
    "\n",
    "try:\n",
    "    tmp = ss.distance.cdist(topo_cells, stream_cells)\n",
    "    ctmp = tmp.min(axis=1)\n",
    "\n",
    "    topo_cells['dist2str'] = ctmp\n",
    "    model_grid = model_grid.join(topo_cells, how='outer', lsuffix='r_', rsuffix='l_')\n",
    "    model_grid.loc[model_grid.obs_type == 'hydro', ['dist2str']] = np.sqrt(L ** 2 / ( 4 * np.pi ))\n",
    "except (ValueError, MemoryError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_grid_cols = [u'segment_len', u'reachcode', u'reach_len', u'reach_int', u'obs_type',\n",
    "                u'lay', u'xcl_', u'ycl_', u'xcr_', u'ycr_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the columns in model_grid and write a GeoTiff of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, item in model_grid.iteritems():\n",
    "    if column not in no_grid_cols:\n",
    "        fname = '{}.tif'.format(column)\n",
    "        dst = os.path.join(model_ws, fname)\n",
    "        if os.path.exists(dst):\n",
    "            os.remove(dst)\n",
    "        data = item.values.reshape(NROW,NCOL)\n",
    "        make_raster(dst, data, NCOL, NROW, gt, shapeproj, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write model_grid to model_grid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(model_ws, 'model_grid')\n",
    "model_file = '{}.csv'.format(model_file)\n",
    "model_grid.to_csv(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ma = np.ma.MaskedArray(model_grid.ned.values.reshape(NROW,NCOL), mask=(ib==0))\n",
    "plt.imshow(model_grid.ned.values.reshape(NROW,NCOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "f2d5c6d16450417aa02fa4289fef68c9": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
